{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc39cf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "from itertools import combinations\n",
    "import ast\n",
    "from matplotlib import gridspec\n",
    "import scipy.stats as stats\n",
    "from statsmodels.stats.multitest import multipletests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a18805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def load_and_prepare(file, predictors_columns):\n",
    "    data_path = os.path.join(base_path, file)\n",
    "    df = pd.read_excel(data_path)\n",
    "    df['Walking Speed'] = df['Walking Speed'].map(walking_speed_mapping).astype(int)\n",
    "    df['Accuracy'] = df['Accuracy'].map(accuracy_mapping).astype(int)\n",
    "    df['Balance'] = df['Balance'].map(balance_mapping).astype(int)\n",
    "    predictors = df[predictors_columns]\n",
    "    return predictors, df\n",
    "\n",
    "# Normalizes outputs to 1\n",
    "def normalize_to_one(df, columns):\n",
    "    normalized_df = df.copy()\n",
    "\n",
    "    for column in columns:\n",
    "        normalized_df[column] = normalized_df[column].astype(float)\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        # Calculate the sum of columns for the current row\n",
    "        row_sum = row[columns].sum()\n",
    "\n",
    "        # Normalize the specified columns\n",
    "        for column in columns:\n",
    "            normalized_df.at[index, column] = row[column] / row_sum\n",
    "\n",
    "    return normalized_df\n",
    "\n",
    "# Reads in only the 27 trials from survey info\n",
    "def read_and_adjust_sheet(filename, sheet_name):\n",
    "\n",
    "    total_rows = 35\n",
    "    rows_to_skip = [1, 2]\n",
    "    nrows = total_rows - len(rows_to_skip) - 6\n",
    "\n",
    "    # Read file, skip irrelevant trials, set first row as header\n",
    "    df = pd.read_excel(filename, sheet_name=sheet_name, skiprows=rows_to_skip, nrows=nrows)\n",
    "\n",
    "    # Reindex the DataFrame from 1 (correct trial numbers)\n",
    "    df.index = range(1, len(df) + 1)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Function to figure out which conditions to hold out for cross val\n",
    "def match_two_parts(condition_x, condition_y):\n",
    "    s_x, a_x, b_x = int(condition_x[1]), int(condition_x[3]), int(condition_x[5])\n",
    "    s_y, a_y, b_y = int(condition_y[1]), int(condition_y[3]), int(condition_y[5])\n",
    "    return ((s_x == s_y) + (a_x == a_y) + (b_x == b_y)) >= 2\n",
    "\n",
    "def agg_arrays(arrays):\n",
    "  return np.concatenate([arr for arr in arrays])\n",
    "\n",
    "# Normalize predictor string column\n",
    "def normalize_predictors(predictor_str):\n",
    "    try:\n",
    "        return set(ast.literal_eval(predictor_str))\n",
    "    except:\n",
    "        return set()\n",
    "    \n",
    "def draw_sig_bracket(ax, x1, x2, y, stars, tick=0.03, lw=1.6, pad=0.01, fs=13):\n",
    "    ax.plot([x1, x1, x2, x2], [y, y + tick, y + tick, y],\n",
    "            color=\"black\", lw=lw, clip_on=False)\n",
    "    ax.text((x1 + x2) / 2, y + pad, stars,\n",
    "            ha=\"center\", va=\"bottom\", fontsize=fs, color=\"black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39763998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "base_path = \"/Users/jordanfeldman/Desktop/Research/Subject data\"\n",
    "filepath_survey = 'Subjective_Responses.xlsx'\n",
    "filepath_survey = os.path.join(base_path, \"Subjective_Responses.xlsx\")\n",
    "predictors_columns = ['Mean Width Straights (mm)', 'Straights Width Variability (mm)',\n",
    "    'Mean Length Straights (mm)', 'Straights Length Variability (mm)', 'Average Speed (m/s)','Head Angle (deg)']\n",
    "targets_columns = ['Balance', 'Foot Placement', 'Walking Speed', 'Metabolics']\n",
    "\n",
    "# Define mappings\n",
    "walking_speed_mapping = {'Slow': 0, 'Medium': 1, 'Fast': 2}\n",
    "accuracy_mapping = {'Low': 0, 'Medium': 1, 'High': 2}\n",
    "balance_mapping = {'Low': 0, 'Medium': 1, 'High': 2}\n",
    "\n",
    "feature_summary = []\n",
    "\n",
    "# Models (only ridge regression, but set up for easy addition)\n",
    "model_dict = {\n",
    "    \"Ridge Regression\": lambda: Ridge(alpha=1.0, random_state=0)\n",
    "}\n",
    "\n",
    "bmh_files = [\n",
    "    'data_BMH01.xlsx', 'data_BMH02.xlsx', 'data_BMH21.xlsx','data_BMH06.xlsx',\n",
    "    'data_BMH07.xlsx', 'data_BMH08.xlsx', 'data_BMH09.xlsx',\n",
    "    'data_BMH10.xlsx', 'data_BMH13.xlsx', 'data_BMH19.xlsx',\n",
    "    'data_BMH20.xlsx', 'data_BMH17.xlsx'\n",
    "]\n",
    "\n",
    "unique_conditions = [f's{x}a{y}b{z}' for x in range(3) for y in range(3) for z in range(3)]\n",
    "model_types = ['subject_agnostic', 'subject_specific']\n",
    "\n",
    "# Loop over model types and models\n",
    "all_results = {mt: {m: [] for m in model_dict} for mt in model_types}\n",
    "subject_specific_importances = {m: defaultdict(lambda: defaultdict(list)) for m in model_dict}\n",
    "subject_specific_coeffs = {m: defaultdict(lambda: defaultdict(list)) for m in model_dict}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a985172b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run models\n",
    "for model_type in model_types:\n",
    "    print(f\"\\n Processing {model_type} \\n\")\n",
    "    scaler = StandardScaler()  # Will be re-fit for each split\n",
    "\n",
    "    if model_type == 'subject_agnostic':\n",
    "        print(model_type)\n",
    "        for test_file in bmh_files:\n",
    "            for test_condition in unique_conditions:\n",
    "                combined_data = pd.DataFrame()\n",
    "                combined_targets = pd.DataFrame()\n",
    "                for bmh_file in bmh_files:\n",
    "                    data_path = os.path.join(base_path, bmh_file)\n",
    "                    df_data = pd.read_excel(data_path)\n",
    "                    df_data['Walking Speed'] = df_data['Walking Speed'].map(walking_speed_mapping).astype(int)\n",
    "                    df_data['Accuracy'] = df_data['Accuracy'].map(accuracy_mapping).astype(int)\n",
    "                    df_data['Balance'] = df_data['Balance'].map(balance_mapping).astype(int)\n",
    "                    df_data['Condition'] = df_data.apply(lambda row: f\"s{row['Walking Speed']:.0f}a{row['Accuracy']:.0f}b{row['Balance']:.0f}\", axis=1)\n",
    "                    predictors = df_data[predictors_columns]\n",
    "\n",
    "                    sheet_name = bmh_file.split('_')[1].replace('.xlsx', '')\n",
    "                    df_survey = read_and_adjust_sheet(filepath_survey, sheet_name)\n",
    "                    df_survey = normalize_to_one(df_survey, targets_columns)\n",
    "                    targets = df_survey[targets_columns]\n",
    "                    is_not_matching = ~df_data['Condition'].apply(lambda cond: match_two_parts(cond, test_condition))\n",
    "                    predictors = predictors[is_not_matching.values]\n",
    "                    targets = targets[is_not_matching.values]\n",
    "\n",
    "                    combined_data = pd.concat([combined_data, predictors], ignore_index=True)\n",
    "                    combined_targets = pd.concat([combined_targets, targets], ignore_index=True)\n",
    "\n",
    "                combined_data_scaled = pd.DataFrame(scaler.fit_transform(combined_data), columns=combined_data.columns)\n",
    "\n",
    "                # Test data\n",
    "                data_path_test = os.path.join(base_path, test_file)\n",
    "                df_data_test = pd.read_excel(data_path_test)\n",
    "                df_data_test['Walking Speed'] = df_data_test['Walking Speed'].map(walking_speed_mapping).astype(int)\n",
    "                df_data_test['Accuracy'] = df_data_test['Accuracy'].map(accuracy_mapping).astype(int)\n",
    "                df_data_test['Balance'] = df_data_test['Balance'].map(balance_mapping).astype(int)\n",
    "                df_data_test['Condition'] = df_data_test.apply(lambda row: f\"s{row['Walking Speed']:.0f}a{row['Accuracy']:.0f}b{row['Balance']:.0f}\", axis=1)\n",
    "                X_test = df_data_test[predictors_columns]\n",
    "                X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "                sheet_name = test_file.split('_')[1].replace('.xlsx', '')\n",
    "                df_survey_test = read_and_adjust_sheet(filepath_survey, sheet_name)\n",
    "                df_survey = normalize_to_one(df_survey, targets_columns)\n",
    "                y_test = df_survey_test[targets_columns]\n",
    "\n",
    "                # Filter for matching conditions in test data\n",
    "                df_data_test = df_data_test.reset_index(drop=True)\n",
    "                is_matching_condition = df_data_test['Condition'] == test_condition\n",
    "                y_test = y_test.reset_index(drop=True)\n",
    "                X_test = X_test.reset_index(drop=True)\n",
    "\n",
    "                X_test = X_test[is_matching_condition]\n",
    "                y_test = y_test[is_matching_condition]\n",
    "\n",
    "                mae_fold = []\n",
    "\n",
    "                for col_idx, column in enumerate(targets_columns):\n",
    "                    for model_name, model_fn in model_dict.items():\n",
    "                        model = model_fn() # new model instance\n",
    "                        model.fit(combined_data_scaled, combined_targets[column])\n",
    "                        predictions = model.predict(X_test)\n",
    "                        for i, value in enumerate(predictions):\n",
    "                            if value < 0:\n",
    "                                predictions[i] = 0\n",
    "                        if col_idx == 0:\n",
    "                            og_predictions_df = pd.DataFrame(predictions, columns=[column])\n",
    "                        else:\n",
    "                            og_predictions_df[column] = predictions\n",
    "\n",
    "                        mae = mean_absolute_error(y_test[column].values, predictions)\n",
    "                        all_results[model_type][model_name].append({\n",
    "                            'test_file': test_file, 'test_condition': test_condition,\n",
    "                            'target_variable': column,\n",
    "                            'mae': mae,\n",
    "                            'predicted_values': predictions.tolist(),\n",
    "                            'actual_values': y_test[column].values.tolist(),\n",
    "                            'predictors_used': ', '.join(predictors_columns)\n",
    "                        })\n",
    "\n",
    "    elif model_type == 'subject_specific':\n",
    "        for bmh_file in bmh_files:\n",
    "            predictors, df_data = load_and_prepare(bmh_file, predictors_columns)\n",
    "            df_data['Condition'] = df_data.apply(lambda row: f\"s{row['Walking Speed']}a{row['Accuracy']}b{row['Balance']}\", axis=1)\n",
    "            sheet_name = bmh_file.split('_')[1].replace('.xlsx', '')  # Remove .xlsx extension\n",
    "            df_survey = read_and_adjust_sheet(filepath_survey, sheet_name)\n",
    "            df_survey = normalize_to_one(df_survey, targets_columns)\n",
    "            targets = df_survey[targets_columns]\n",
    "            subject_maes = []\n",
    "            for test_condition in df_data['Condition'].unique():\n",
    "                targets_drop = targets.reset_index(drop=True)\n",
    "                is_not_matching = ~(df_data['Condition'] == test_condition)\n",
    "                X_train = predictors[df_data['Condition'] != test_condition]\n",
    "                X_test = predictors[df_data['Condition'] == test_condition]\n",
    "                y_train = targets_drop[df_data['Condition'] != test_condition]\n",
    "                y_test = targets_drop[df_data['Condition'] == test_condition]\n",
    "                scaler = StandardScaler()\n",
    "                X_train_scaled = scaler.fit_transform(X_train)\n",
    "                X_test_scaled = scaler.transform(X_test)\n",
    "                for target_col in targets_columns:\n",
    "                    for model_name, model_fn in model_dict.items():\n",
    "                        model = model_fn() # new model instance\n",
    "                        model.fit(X_train_scaled, y_train[target_col])\n",
    "                        predictions = model.predict(X_test_scaled)\n",
    "                        predictions = np.clip(predictions, 0, None)\n",
    "                        mae = mean_absolute_error(y_test[target_col], predictions)\n",
    "                        all_results['subject_specific'][model_name].append({\n",
    "                            'subject': bmh_file.split('_')[1],\n",
    "                            'test_condition': test_condition,\n",
    "                            'target_variable': target_col,\n",
    "                            'mae': mae,\n",
    "                            'predicted_values': predictions.tolist(),\n",
    "                            'actual_values': y_test[target_col].tolist(),\n",
    "                            'predictors_used': ', '.join(predictors_columns)\n",
    "                        })\n",
    "                        subj = bmh_file.split('_')[1]\n",
    "                        # Only subject specific: importances & coeffs\n",
    "                        if X_train_scaled.shape[0]>=2 and len(np.unique(y_train[target_col]))>1:\n",
    "                            result = permutation_importance(model, X_train_scaled, y_train[target_col], n_repeats=30, random_state=0)\n",
    "                            subject_specific_importances[model_name][subj][target_col].append(result.importances_mean)\n",
    "                            feature_summary.append({\n",
    "                                'subject': subj,\n",
    "                                'target': target_col,\n",
    "                                'model': model_name,\n",
    "                                'predictors': predictors_columns,\n",
    "                                'feature_names': X_train.columns.tolist(),\n",
    "                                'feature_importances': result.importances_mean.tolist(),\n",
    "                                'coefficients': model.coef_.tolist() if hasattr(model, 'coef_') else None\n",
    "                                \n",
    "                            })\n",
    "                        subject_specific_coeffs[model_name][subj][target_col].append(model.coef_)\n",
    "# Save all model results in one CSV per model type\n",
    "for model_type in all_results:\n",
    "    for model_name in all_results[model_type]:\n",
    "        df = pd.DataFrame(all_results[model_type][model_name])\n",
    "        outfile = f\"{model_type}_results_{model_name.replace(' ','_').lower()}_rerun.csv\"\n",
    "        df.to_csv(outfile, index=False)\n",
    "        print(f\"Saved master results CSV: {outfile}\")\n",
    "\n",
    "# Save the coefficients and importances summary\n",
    "feature_df = pd.DataFrame(feature_summary)\n",
    "feature_df.to_csv(\"subject_specific_feature_summary.csv\", index=False)\n",
    "print(\"Saved feature importance & coefficients CSV: subject_specific_feature_summary.csv\")\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8430d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subject agnostic errors\n",
    "cv_path = '/Users/jordanfeldman/Desktop/Research/subject_agnostic_results_ridge_regression.csv'\n",
    "\n",
    "threshold = 0.11       \n",
    "use_thresh = 'y'        # 'y' to use threshold, 'n' to ignore\n",
    "\n",
    "# Load & clean\n",
    "cv_df = pd.read_csv(cv_path)\n",
    "cv_df['predicted_values'] = cv_df['predicted_values'].apply(lambda x: float(str(x).strip('[]')))\n",
    "cv_df['actual_values']    = cv_df['actual_values'].apply(lambda x: float(str(x).strip('[]')))\n",
    "\n",
    "pair_indices = list(combinations(range(4), 2))\n",
    "\n",
    "# Function to get sign match with/without threshold\n",
    "def pair_match(pred_diff, act_diff, k, use_thresh_flag):\n",
    "    if use_thresh_flag == 'y':\n",
    "        if (abs(act_diff) < k and abs(pred_diff) < k) or (np.sign(pred_diff) == np.sign(act_diff)):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        if act_diff == 0 and pred_diff == 0:\n",
    "            return 1\n",
    "        elif act_diff == 0 and pred_diff != 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return int(np.sign(pred_diff) == np.sign(act_diff))\n",
    "\n",
    "cv_errors = []\n",
    "k = float(threshold)\n",
    "\n",
    "for i in range(0, len(cv_df), 4):\n",
    "    block = cv_df.iloc[i:i+4].reset_index(drop=True)\n",
    "    if len(block) < 4 or len(block['target_variable'].unique()) < 4: \n",
    "        continue\n",
    "\n",
    "    block_matches = []\n",
    "    for idx1, idx2 in pair_indices:\n",
    "        pred_diff = block.loc[idx1, 'predicted_values'] - block.loc[idx2, 'predicted_values']\n",
    "        act_diff  = block.loc[idx1, 'actual_values']    - block.loc[idx2, 'actual_values']\n",
    "        block_matches.append(pair_match(pred_diff, act_diff, k, use_thresh))\n",
    "\n",
    "    if block_matches:\n",
    "        acc = np.mean(block_matches) * 100.0\n",
    "        err = 100.0 - acc\n",
    "        cv_errors.append(err)\n",
    "\n",
    "print(f\"Subject agnostic errors (n={len(cv_errors)}):\")\n",
    "print(cv_errors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bf385a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subject specific errors\n",
    "ss_path = '/Users/jordanfeldman/Desktop/Research/subject_specific_results_ridge_regression.csv'\n",
    "\n",
    "threshold = 0.11\n",
    "use_thresh = 'y'   # 'y' to use threshold, 'n' to ignore\n",
    "pair_indices = list(combinations(range(4), 2))\n",
    "\n",
    "# Load & clean\n",
    "df = pd.read_csv(ss_path)\n",
    "df['predicted_values'] = df['predicted_values'].apply(lambda x: float(str(x).strip('[]')))\n",
    "df['actual_values']    = df['actual_values'].apply(lambda x: float(str(x).strip('[]')))\n",
    "\n",
    "new_df = df.copy()\n",
    "# Function to get sign match with/without threshold\n",
    "def pair_match(pred_diff, act_diff, k, use_thresh_flag):\n",
    "    if use_thresh_flag == 'y':\n",
    "        if (abs(act_diff) < k and abs(pred_diff) < k) or (np.sign(pred_diff) == np.sign(act_diff)):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        if act_diff == 0 and pred_diff == 0:\n",
    "            return 1\n",
    "        elif act_diff == 0 and pred_diff != 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return int(np.sign(pred_diff) == np.sign(act_diff))\n",
    "\n",
    "ss_errors = []  \n",
    "k = float(threshold)\n",
    "\n",
    "# If your CSV has a 'subject' column, this groups by subject; otherwise, this still works as one group\n",
    "grouped = new_df.groupby(['subject'], dropna=False) if 'subject' in new_df.columns else [('all', new_df)]\n",
    "\n",
    "for subject, group in grouped:\n",
    "    for i in range(0, len(group), 4):\n",
    "        block = group.iloc[i:i+4].reset_index(drop=True)\n",
    "        if len(block) < 4 or len(block['target_variable'].unique()) < 4:\n",
    "            continue\n",
    "\n",
    "        block_matches = []\n",
    "        for idx1, idx2 in pair_indices:\n",
    "            pred_diff = block.loc[idx1, 'predicted_values'] - block.loc[idx2, 'predicted_values']\n",
    "            act_diff  = block.loc[idx1, 'actual_values']    - block.loc[idx2, 'actual_values']\n",
    "            block_matches.append(pair_match(pred_diff, act_diff, k, use_thresh))\n",
    "\n",
    "        if block_matches:\n",
    "            acc = np.mean(block_matches) * 100.0\n",
    "            err = 100.0 - acc\n",
    "            ss_errors.append(err)\n",
    "\n",
    "print(f\"Subject-specific errors (n={len(ss_errors)}):\")\n",
    "print(ss_errors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481dac51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permutation importance plotting script\n",
    "\n",
    "# Load the file\n",
    "df = pd.read_csv(\"/Users/jordanfeldman/Desktop/Research/subject_specific_feature_summary.csv\")\n",
    "df_full = df.copy()\n",
    "\n",
    "# Extract long-format data for plotting\n",
    "long_data = []\n",
    "\n",
    "for _, row in df_full.iterrows():\n",
    "    try:\n",
    "        feature_names = ast.literal_eval(row['feature_names'])\n",
    "        importances = ast.literal_eval(row['feature_importances'])\n",
    "        target = row['target']  # One target per row\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    for name, score in zip(feature_names, importances):\n",
    "        long_data.append({\n",
    "            'Predictor': name.strip(),\n",
    "            'Importance': score,\n",
    "            'Target': target\n",
    "        })\n",
    "\n",
    "# Create plot DataFrame\n",
    "plot_df = pd.DataFrame(long_data)\n",
    "\n",
    "pretty_map = {\n",
    "    'Mean Width Straights (mm)': 'Step width',\n",
    "    'Straights Width Variability (mm)': 'Step width\\nvariability',\n",
    "    'Mean Length Straights (mm)': 'Step length',\n",
    "    'Straights Length Variability (mm)': 'Step length\\nvariability',\n",
    "    'Average Speed (m/s)': 'Walking\\nspeed',\n",
    "    'Head Angle (deg)': 'Head\\nangle',\n",
    "}\n",
    "display_order = [\n",
    "    'Step width',\n",
    "    'Step width\\nvariability',\n",
    "    'Step length',\n",
    "    'Step length\\nvariability',\n",
    "    'Walking\\nspeed',\n",
    "    'Head\\nangle'\n",
    "]\n",
    "\n",
    "plot_df = plot_df.copy()\n",
    "plot_df['Predictor'] = plot_df['Predictor'].map(pretty_map).fillna(plot_df['Predictor'])\n",
    "plot_df['Predictor'] = pd.Categorical(plot_df['Predictor'], categories=display_order, ordered=True)\n",
    "\n",
    "hue_order = ['Walking Speed', 'Balance', 'Foot Placement', 'Metabolics']\n",
    "green_palette = {\n",
    "    'Walking Speed':  '#E6F4D7',  # very light green\n",
    "    'Balance':        '#A4B999',  # light green\n",
    "    'Foot Placement': '#6FAA5F',  # dark green\n",
    "    'Metabolics':     '#B6D64A',  # yellow-green\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(10, 9))\n",
    "\n",
    "ax = sns.barplot(\n",
    "    data=plot_df,\n",
    "    x='Predictor',\n",
    "    y='Importance',\n",
    "    hue='Target',\n",
    "    hue_order=hue_order,\n",
    "    errorbar=None,              \n",
    "    palette=[green_palette[h] for h in hue_order],\n",
    "    edgecolor='black',             \n",
    "    linewidth=1.2\n",
    ")\n",
    "\n",
    "ax.set_xlabel('Features for subject-specific model', fontsize=20)\n",
    "ax.set_ylabel('Feature relevance', fontsize=20)\n",
    "ax.tick_params(axis='x', labelsize=16)\n",
    "ax.tick_params(axis='y', labelsize=16)\n",
    "ax.set_ylim(0, 0.6) \n",
    "sns.despine(top=True, right=True)\n",
    "\n",
    "leg = ax.legend(title='Movement goals', frameon=False, labels =['Walking speed', 'Balance', 'Foot placement', 'Energy expenditure'])\n",
    "plt.setp(leg.get_title(), fontsize=16)\n",
    "for txt in leg.get_texts():\n",
    "    txt.set_fontsize(16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('permutation_importance_redo.svg', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5932df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranking error box plots with proportions of error levels\n",
    "cv_df_plot = pd.DataFrame({'Error (%)': cv_errors, 'Condition': 'Cross Validation'})\n",
    "ss_df_plot = pd.DataFrame({'Error (%)': ss_errors, 'Condition': 'Subject Specific'})\n",
    "plot_df = pd.concat([cv_df_plot, ss_df_plot], ignore_index=True)\n",
    "plot_df = plot_df.copy()\n",
    "plot_df[\"Condition\"] = plot_df[\"Condition\"].replace({\n",
    "    \"Subject agnostic\": \"New subject, new condition\",\n",
    "    \"Subject specific\": \"Subject-specific, new condition\"\n",
    "})\n",
    "order = [\"New subject, new condition\", \"Subject-specific, new condition\"]\n",
    "plot_df[\"Condition\"] = plot_df[\"Condition\"].replace({\n",
    "    \"Cross Validation\": \"New subject, new condition\",\n",
    "    \"Subject Specific\": \"Subject-specific, new condition\"\n",
    "})\n",
    "\n",
    "# box colors\n",
    "palette = {\n",
    "    \"New subject, new condition\": \"lightgrey\",\n",
    "    \"Subject-specific, new condition\": \"#32572C\"\n",
    "}\n",
    "\n",
    "# Stats\n",
    "shapiro_cv = stats.shapiro(cv_errors)\n",
    "shapiro_ss = stats.shapiro(ss_errors)\n",
    "print(f\"Shapiro-Wilk CV errors: W={shapiro_cv.statistic}, p-value={shapiro_cv.pvalue}\")\n",
    "print(f\"Shapiro-Wilk SS errors: W={shapiro_ss.statistic}, p-value={shapiro_ss.pvalue}\")\n",
    "\n",
    "wilcoxon_test = stats.wilcoxon(cv_errors, ss_errors)\n",
    "print(f\"Wilcoxon p-value: {wilcoxon_test[1]}\")\n",
    "\n",
    "p_adj = float(np.asarray(wilcoxon_test[1]))\n",
    "stars = \"**\" if p_adj < 0.005 else \"*\" if p_adj < 0.05 else None\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(12, 10), constrained_layout=False)\n",
    "gs  = fig.add_gridspec(\n",
    "    nrows=2, ncols=2,\n",
    "    width_ratios=[1, 1],\n",
    "    height_ratios=[0.22, 1.0],  \n",
    "    wspace=0.35, hspace=0.05\n",
    ")\n",
    "\n",
    "ax_box   = fig.add_subplot(gs[:, 0])   \n",
    "ax_leg   = fig.add_subplot(gs[0, 1])  \n",
    "ax_stack = fig.add_subplot(gs[1, 1])  \n",
    "ax_leg.axis(\"off\")\n",
    "\n",
    "# Box plot\n",
    "sns.boxplot(\n",
    "    data=plot_df,\n",
    "    x=\"Condition\",\n",
    "    y=\"Error (%)\",\n",
    "    showcaps=True,\n",
    "    ax=ax_box,\n",
    "    legend=False,\n",
    "    palette=palette,\n",
    "    showfliers=False\n",
    ")\n",
    "\n",
    "for patch in ax_box.patches:\n",
    "    patch.set_edgecolor(\"black\")\n",
    "\n",
    "for line in ax_box.lines:\n",
    "    line.set_color(\"black\")\n",
    "\n",
    "ax_box.tick_params(axis='both', labelsize=16)\n",
    "\n",
    "ax_box.set_xticklabels(\n",
    "    [\"Subject-agnostic\", \"Subject-specific\"],\n",
    "    fontsize=16\n",
    ")\n",
    "ax_box.set_xlabel(\"Model type\", fontsize=20)\n",
    "\n",
    "# add mean markers\n",
    "means = plot_df.groupby(\"Condition\")[\"Error (%)\"].mean().reindex(order)\n",
    "xpos = np.arange(len(order))\n",
    "ax_box.scatter(xpos, means.values, color=\"black\", marker=\"^\", s=85, zorder=3)\n",
    "\n",
    "# add significance\n",
    "if stars:\n",
    "    ymax = plot_df[\"Error (%)\"].max()\n",
    "    ymin = plot_df[\"Error (%)\"].min()\n",
    "    span = ymax - ymin\n",
    "    y_br = ymax + 0.10 * span - 20\n",
    "    draw_sig_bracket(ax_box, 0, 1, y_br, stars,\n",
    "                     tick=0.035 * span, pad=0.05 * span)\n",
    "    ax_box.set_ylim(top=y_br + 0.12 * span)\n",
    "\n",
    "ax_box.set_ylabel(\"Ranking error (%)\", fontsize=20)\n",
    "ax_box.set_ylim(0, 100)\n",
    "sns.despine(ax=ax_box)\n",
    "\n",
    "# Stacked proportions \n",
    "tmp = plot_df.copy()\n",
    "level_order = [0.0, 16.7, 33.3, 50.0, 66.7, 83.3]  \n",
    "tmp[\"ValueRounded\"] = tmp[\"Error (%)\"].round(1)\n",
    "levels_obs = [lv for lv in level_order if lv in tmp[\"ValueRounded\"].unique()]\n",
    "\n",
    "\n",
    "counts = (tmp.groupby([\"Condition\", \"ValueRounded\"])[\"Error (%)\"]\n",
    "            .count()\n",
    "            .unstack(fill_value=0)\n",
    "            .reindex(index=order, columns=levels_obs))\n",
    "props = counts.div(counts.sum(axis=1), axis=0)\n",
    "\n",
    "\n",
    "level_colors = {\n",
    "    0.0:  \"#4B2E83\",  # deep purple\n",
    "    16.7: \"#325AA8\",  # blue\n",
    "    33.3: \"#1E7F88\",  # teal\n",
    "    50.0: \"#18A7AE\",  # aqua\n",
    "    66.7: \"#27C5D8\",  # light cyan\n",
    "    83.3: \"#BFE7E9\",  # very light cyan\n",
    "}\n",
    "\n",
    "bottom = np.zeros(len(order))\n",
    "for lvl in levels_obs:\n",
    "    ax_stack.bar(\n",
    "        order,\n",
    "        props[lvl].values,\n",
    "        bottom=bottom,\n",
    "        color=level_colors[lvl],\n",
    "        width=0.65,\n",
    "        edgecolor=\"none\",\n",
    "        label=f\"{lvl:.1f}%\"\n",
    "    )\n",
    "    bottom += props[lvl].values\n",
    "\n",
    "ax_stack.set_ylim(0, 1.0)\n",
    "ax_stack.set_ylabel(\"Proportion of ranking error\", fontsize=20)\n",
    "ax_stack.set_xlabel(\"Model type\", fontsize=20)\n",
    "ax_stack.set_xticklabels(\n",
    "    [\"Subject-agnostic\", \"Subject-specific\"],\n",
    "    fontsize=16\n",
    ")\n",
    "ax_stack.tick_params(axis='both', labelsize=16)\n",
    "sns.despine(ax=ax_stack)\n",
    "\n",
    "\n",
    "handles, labels = ax_stack.get_legend_handles_labels()\n",
    "label_to_handle = {lab: h for h, lab in zip(handles, labels)}\n",
    "ordered_labels  = [f\"{lv:.1f}%\" for lv in level_order if f\"{lv:.1f}%\" in label_to_handle]\n",
    "ordered_handles = [label_to_handle[lab] for lab in ordered_labels]\n",
    "leg = ax_leg.legend(\n",
    "    ordered_handles,\n",
    "    ordered_labels,\n",
    "    title=\"Ranking error (%)\",\n",
    "    ncol=3,             \n",
    "    loc=\"center\",\n",
    "    frameon=False,\n",
    "    fontsize=16,\n",
    "    handlelength=1.0,\n",
    "    columnspacing=1.2,\n",
    "    labelspacing=0.6,\n",
    ")\n",
    "leg.get_title().set_fontsize(16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ranking_error_boxplots_fixed.svg', dpi=300)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs109a",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
